{"posts":[{"title":"2026春节AI公众号观察：从“发红包”到“让AI办事”","text":"写在前面这篇是我对最近两次春节（重点是 2026 年春节，补充 2025 年春节）里，微信公众号与相关转载内容中“大模型/AI”讨论的一次整理。 说明两点： 公众号原文有些需要微信内访问，本文优先引用可公开访问的网页； 部分信息来自“媒体转载公众号”，我会在文内明确写出“原公众号”与“转载来源”。 一、时间线：春节窗口期，AI明显加速2025 年春节前后（上一轮爆发） 2025-01-20：DeepSeek-R1 发布（行业普遍视为春节前后讨论引爆点之一）。参考： DeepSeek-R1 GitHub 投资界春节回顾（新浪转载） 2025-02-06：投资界发布春节半月回顾，集中盘点 DeepSeek、豆包、阶跃星辰、智谱等进展。参考： AI大模型春节「爆发」回顾 2026 年春节前后（“应用打法”更重） 2026-01-05：据“腾讯混元”公众号消息，微信小程序推出“AI应用及线上工具小程序成长计划”，强调 token、云开发、流量激励。参考： 搜狐转载（来源：上海证券报） 2026-02-06：投资界转载“象先志”公众号文章，核心观点是：春节 AI 战不只是拉新，关键是能否把用户习惯沉淀下来。参考： 春节AI大战「杀疯了」 2026-02-13：投资界转载“猎云网”公众号文章，总结“春节档密集发布 + 大厂投入加码”的竞争态势。参考： 国产大模型发起春节攻势 二、这轮公众号讨论里，反复出现的 4 个关键词1）“红包战”还在，但目标变了2026 年春节讨论里，红包/免单仍是高频词，但很多分析不再只看短期 DAU，而是看能否把一次性活动转成持续使用。这点在“象先志”相关讨论里很典型：流量只是入口，习惯才是壁垒。 2）从“聊天能力”走向“执行能力”今年我看到的一个明显变化是：公众号里对 AI 的评价标准，开始从“会不会答题”转向“能不能办事”。典型叙事是：接入交易、出行、内容处理、工作流后，AI 才可能成为稳定入口。 3）微信生态从“观望”到“给工具”从 2026-01-05 的成长计划看，微信侧动作更像在做“基础设施”：给开发者模型额度、云开发能力、流量激励，目的是把更多 AI 功能直接沉淀在小程序里。 4）“春节档”成为 AI 产品的压力测试场春节的特点是高流量、高社交传播和高场景密度。所以公众号里对春节 AI 项目的评判，越来越像一句话：不只要会演示，还要扛得住真实使用峰值。 三、我整理的“公众号样本池”（便于二次追踪）这次可公开检索到、且和春节 AI 讨论高度相关的样本里，建议重点跟踪这几类账号： 平台/产品官方号：如“腾讯混元”“阿里巴巴”等（偏官方动作发布）。 科技商业观察号：如“猎云网”“象先志”“AI新榜”等（偏策略、竞争格局、实测体验）。 财经媒体转载链路：投资界、每经、上证/新浪等（优点是网页公开可访问，便于归档）。 补充参考： 微信，大动作！（每经，2025-04-17） AI新榜实测腾讯系接入DeepSeek（转载页） 四、我的判断：春节后的 AI 竞争，将进入“留存效率战”如果只看春节周期，我认为最重要的变化是： 大模型竞争重心，从参数/榜单继续向“场景完成率”移动； 公众号内容的叙事重点，从“技术惊艳”转向“业务闭环”； 下一阶段真正拉开差距的，不是再做一个聊天框，而是把 AI 深度嵌入现有生态链路（支付、内容、工具、服务）。 简化成一句话：春节是拉新，节后看留存；能办成事，才有复利。 参考链接（按文中出现顺序） DeepSeek-R1 GitHub AI大模型春节「爆发」回顾，阿里、DeepSeek齐上桌（新浪财经） 微信发布AI小程序成长计划（搜狐，来源上海证券报） 春节AI大战「杀疯了」（投资界，标注原公众号“象先志”） 国产大模型发起春节攻势（投资界，标注原公众号“猎云网”） 微信，大动作！（每日经济新闻） 腾讯产品疯狂接入DeepSeek，哪个最好用？| AI新榜实测（转载页）","link":"/2026/02/20/spring-festival-ai-wechat-summary/"},{"title":"如何在 OpenClaw 中接入企业微信：插件选择、配置与避坑","text":"写在前面这篇文章不走“拍脑袋教程”路线，而是基于我对真实项目与代码仓库的检索来写： 先确认 OpenClaw 官方主仓 的 WeCom 能力现状； 再对比 社区插件 的安装方式、配置字段和适用场景； 最后给出一套“今天就能落地”的接入方案。 如果你想的是“OpenClaw + 企业微信到底该装哪个插件、怎么配才稳”，这篇就够用。 一、我检索了哪些实际项目为了避免信息失真，我主要查了这几类来源： OpenClaw 官方仓库（openclaw/openclaw） 中国生态插件仓库（BytePioneer-AI/openclaw-china） 独立社区插件仓库（sunnoy/openclaw-plugin-wecom） 检索到的关键事实（可复核）： BytePioneer-AI/openclaw-china 的 README 明确给出了安装命令： openclaw plugins install @openclaw-china/wecom-app openclaw plugins install @openclaw-china/wecom 同仓库的 doc/guides/wecom-app/configuration.md 给出了 channels.wecom-app.* 的完整配置项（如 corpId、corpSecret、agentId、token、encodingAESKey）。 同仓库 extensions/wecom-app/openclaw.plugin.json 显示该插件 channel id 为 wecom-app，定位是“企业微信自建应用消息渠道（支持主动推送）”。 sunnoy/openclaw-plugin-wecom 的 package.json 显示 npm 包名是 @sunnoy/wecom，channel id 为 wecom。 OpenClaw 主仓存在 WeCom 接入 PR（openclaw/openclaw#13228），但 PR 页面显示仍在讨论/未并入主线（写文时状态）。 一句话总结：当前最稳妥的是用社区成熟插件，官方主线能力可持续关注但不建议当作唯一依赖。 二、先选插件：不是“哪个好”，而是“哪个适合你”1）@openclaw-china/wecom-app（推荐默认）适合： 你要走 企业微信自建应用 模式； 你需要 主动发送消息（不是只被动回消息）； 主要是 1v1 或内部应用消息链路。 优点： 配置文档完整； 字段语义清晰，和企业微信后台参数一一对应； 在中文社区里案例最多，排障资料也更容易找。 2）@openclaw-china/wecom适合： 你更偏向“机器人”形态； 需要群聊触达能力（具体以插件当前版本说明为准）。 3）@sunnoy/wecom适合： 你希望用独立社区维护的 WeCom channel； 你能接受插件生态差异，并自己做版本回归验证。 我建议： 企业内部落地优先：先上 wecom-app； 后续再根据业务扩展到其他 channel，不要一开始就“全渠道一起上”。 三、最小可用接入方案（按 wecom-app）下面这套是我认为最适合大多数团队的“先跑通再优化”路径。 Step 1：安装插件1openclaw plugins install @openclaw-china/wecom-app Step 2：在企业微信后台准备参数进入企业微信管理后台，创建自建应用，拿到这些值： CorpId AgentId Secret API 回调的 Token API 回调的 EncodingAESKey Step 3：写入 OpenClaw 配置官方社区文档给出的 CLI 配置方式如下（核心字段）： 1234567openclaw config set channels.wecom-app.enabled trueopenclaw config set channels.wecom-app.webhookPath /wecom-appopenclaw config set channels.wecom-app.token your-tokenopenclaw config set channels.wecom-app.encodingAESKey your-43-char-encoding-aes-keyopenclaw config set channels.wecom-app.corpId your-corp-idopenclaw config set channels.wecom-app.corpSecret your-app-secretopenclaw config set channels.wecom-app.agentId 1000002 如果你习惯直接改 openclaw.json，可以按这个最小模板： 12345678910111213{ &quot;channels&quot;: { &quot;wecom-app&quot;: { &quot;enabled&quot;: true, &quot;webhookPath&quot;: &quot;/wecom-app&quot;, &quot;token&quot;: &quot;your-token&quot;, &quot;encodingAESKey&quot;: &quot;your-43-char-encoding-aes-key&quot;, &quot;corpId&quot;: &quot;your-corp-id&quot;, &quot;corpSecret&quot;: &quot;your-app-secret&quot;, &quot;agentId&quot;: 1000002 } }} Step 4：配置回调 URL 并联调企业微信“接收消息”里 URL 要和 webhookPath 对应，例如： https://your-domain.com/wecom-app 联调建议按这个顺序： 先验证 URL 可达（公网 / 反向代理 / 证书） 再验证 Token 与 AESKey 是否一致 最后才测消息收发（避免把网络问题误判成插件问题） 四、这几个坑最容易卡住1）公网可达性没打通企业微信回调是平台主动调用你，内网地址肯定不行。最常见问题是： 没有公网入口； 网关端口没放行； Nginx/Traefik 反代路径没对齐 webhookPath。 2）把插件名和 channel id 搞混 插件包名是安装维度（如 @openclaw-china/wecom-app）； channels.wecom-app 是配置维度。 两者不一致就会出现“安装成功但渠道不生效”的假象。 3）直接押注“官方 PR 能力”官方主仓的 WeCom 能力确实在推进，但如果你要本周上线，建议优先选已在社区验证过的插件版本，把可控性握在自己手里。 五、给团队的落地建议（可直接照抄）如果你是第一次接 OpenClaw + 企业微信，我建议按这个节奏做： 第 1 天：先跑通 wecom-app 最小链路（收消息 + 回消息）； 第 2 天：补日志、告警、重试策略； 第 3 天：再做多账号、语音识别、群聊等增强。 不要一上来就追求“功能全开”。在 IM 场景里，稳定可回消息 永远比“功能很多但偶发掉线”更重要。 参考链接（可复核） BytePioneer-AI/openclaw-china README wecom-app 配置指南 wecom-app 插件清单（openclaw.plugin.json） sunnoy/openclaw-plugin-wecom package.json OpenClaw 主仓 WeCom PR #13228","link":"/2026/02/24/openclaw-wecom-integration-guide/"},{"title":"Agent Skill 脚本路径问题：四大 CLI 实现对比与最佳实践","text":"写在前面在写 Agent Skill 时，有一个问题几乎每个人都会踩到： 脚本路径怎么写？是绝对路径还是相对路径？相对谁？如果 skill 既可以用户级安装，也可以项目级安装，怎么保证两种情况都能跑？ 这篇文章不是基于猜测，而是直接读了四个主流 CLI 的源码： Claude Code（@anthropic-ai/claude-code v2.1.39）—— Anthropic 官方 Codex（@openai/codex v0.104.0）—— OpenAI 官方，Rust 编译的原生二进制 oh-my-opencode（npx 版本）—— 社区最流行的 opencode 增强层 openclaw（v2026.2.6-3）—— 另一个社区 CLI，内嵌 pi-coding-agent 一、什么是 Agent Skill？Skill 是以目录为单位的”能力包”，最少只需要一个 SKILL.md 文件： 12345my-skill/├── SKILL.md ← 必须，包含 name/description 和指令├── scripts/ ← 可选，执行脚本├── references/ ← 可选，参考文档└── assets/ ← 可选，模板/资源 SKILL.md 格式固定： 123456789---name: my-skilldescription: 这个 skill 做什么、什么时候用。---## 指令执行脚本：scripts/run.py 四家 CLI 都遵循 agentskills.io 这个开放规范，结构是通用的。 二、各 CLI 的安装路径Skill 可以安装在不同层级，优先级通常是项目级 &gt; 用户级 &gt; 内置： Scope Claude Code Codex oh-my-opencode openclaw 项目级 .claude/skills/ .agents/skills/（向上遍历到 git root） .opencode/skills/ &lt;workspace&gt;/skills/ 用户级 ~/.claude/skills/ ~/.agents/skills/ ~/.config/opencode/skills/ ~/.openclaw/skills/ 系统级 — /etc/codex/skills/ — — 内置 随包发布 随包发布 随包发布 随包发布 关键点：同名 skill，项目级优先。这意味着团队可以在 repo 里覆盖全局 skill。 三、各 CLI 的路径注入机制（源码层面）这是这篇文章的核心。 3.1 Claude Code：注入 baseDir 字符串Claude Code 在每次调用 skill 时，会把 skill 的绝对路径注入到 LLM 上下文： 123456// claude-code/cli.js（v2.1.39）const skillBlock = `&lt;skill name=&quot;${skill.name}&quot; location=&quot;${skill.filePath}&quot;&gt;References are relative to ${skill.baseDir}.${body}&lt;/skill&gt;`; LLM 收到的上下文形如： 12345&lt;skill name=&quot;my-skill&quot; location=&quot;/Users/nic/.claude/skills/my-skill/SKILL.md&quot;&gt;References are relative to /Users/nic/.claude/skills/my-skill.（SKILL.md 正文）&lt;/skill&gt; 结论：不需要写任何路径变量。SKILL.md 里直接写 scripts/run.py，LLM 会根据注入的 baseDir 自行补全成绝对路径。 3.2 Codex：注入 file path，模型自己读文件Codex 是 Rust 写的原生二进制，system prompt 里注入的是这段话： 12345678## SkillsA skill is a set of local instructions to follow that is stored in a `SKILL.md` file.Below is the list of skills that can be used. Each entry includes a name, description,and file path so you can open the source for full instructions when using a specific skill.### How to use skills- Discovery: The list above is the skills available in this session (name + description + file path). Skill bodies live on disk at the listed paths. 每个 skill 都附带绝对 file path，模型需要时会用 read 工具去读完整的 SKILL.md。这是渐进式加载（progressive disclosure）： 启动时只加载 name + description + 路径（约 100 tokens） 决定使用某个 skill 时，才读完整 SKILL.md（&lt; 5000 tokens） 需要时才读 scripts/、references/ 里的文件 结论：SKILL.md 里写相对路径即可，Codex 会把 skill 的绝对路径告诉模型，模型能算出 scripts/ 的完整路径。 3.3 oh-my-opencode：@path 语法，代码层面替换oh-my-opencode 是唯一做代码级字符串替换的实现： 1234567// src/shared/skill-path-resolver.tsfunction resolveSkillPathReferences(content, basePath) { return content.replace( /@([a-zA-Z0-9_-]+\\/[a-zA-Z0-9_.\\-\\/]*)/g, (_, relativePath) =&gt; join(basePath, relativePath) );} 同时在 system prompt 里注入说明： 12Base directory for this skill: /absolute/path/to/skill/File references (@path) in this skill are relative to this directory. 结论：如果你写的 skill 专门面向 oh-my-opencode，可以用 @scripts/run.py 语法，会被硬替换成绝对路径，100% 可靠，不依赖模型推理。 3.4 openclaw：{baseDir} 变量 + 模型推理openclaw 的文档里写了”Use {baseDir} in instructions to reference the skill folder path”，但源码（pi-coding-agent）的实际实现是： 12345678910111213// pi-coding-agent/dist/core/skills.js// formatSkillsForPrompt 输出`&lt;available_skills&gt; &lt;skill&gt; &lt;name&gt;my-skill&lt;/name&gt; &lt;description&gt;...&lt;/description&gt; &lt;location&gt;/absolute/path/to/SKILL.md&lt;/location&gt; &lt;/skill&gt;&lt;/available_skills&gt;`// 附带提示语：&quot;When a skill file references a relative path, resolve it against the skill directory (parent of SKILL.md / dirname of the path) and use that absolute path in tool commands.&quot; 没有代码替换 {baseDir}。{baseDir} 的实际效果依赖模型从 &lt;location&gt; 字段推算出父目录，然后自行理解。 结论：{baseDir} 可以写，模型一般能理解，但这是”prompt 约定”不是”代码保证”。 四、四家对比总结 CLI 路径机制 {baseDir} 变量 相对路径写法 可靠性 Claude Code 注入 References are relative to &lt;绝对路径&gt; ❌ 无此变量 直接写 scripts/foo.py ✅ 模型理解稳定 Codex 注入每个 skill 的绝对 file path ❌ 无此变量 直接写 scripts/foo.py ✅ 模型理解稳定 oh-my-opencode 代码级替换 @path → 绝对路径 ❌（用 @path） 写 @scripts/foo.py ✅ 确定性替换 openclaw 注入 &lt;location&gt; + 提示语 ⚠️ 文档写了但靠模型 写 {baseDir}/scripts/foo.py 或相对路径 ⚠️ 依赖模型推理 五、最佳实践5.1 agentskills.io 官方规范的建议agentskills.io 规范 明确写道： When referencing other files in your skill, use relative paths from the skill root 12Run the extraction script:scripts/extract.py 不写任何变量，直接写相对路径。所有 CLI 都会把 SKILL.md 的绝对路径告知模型，模型有能力补全出 scripts/ 的完整路径。 5.2 脚本内部：用语言原生方式定位自身无论哪个 CLI，脚本被执行时的 working directory 是不固定的（通常是用户的项目目录，不是 skill 目录）。所以脚本内部不能用相对路径读同目录下的文件，要用语言原生方式： Python： 123456789import osfrom pathlib import Path# 定位脚本所在目录（即 skill 的 scripts/ 目录）SCRIPT_DIR = Path(__file__).resolve().parentSKILL_DIR = SCRIPT_DIR.parent # scripts/ 的上一级就是 skill 根目录# 安全读取同目录下的配置文件config_path = SKILL_DIR / &quot;assets&quot; / &quot;config.json&quot; Shell： 1234567#!/bin/bash# 定位脚本所在目录SCRIPT_DIR=&quot;$(cd &quot;$(dirname &quot;$0&quot;)&quot; &amp;&amp; pwd)&quot;SKILL_DIR=&quot;$(dirname &quot;$SCRIPT_DIR&quot;)&quot;# 安全读取同目录下的资源source &quot;$SKILL_DIR/references/helpers.sh&quot; 5.3 兼容三家的写法如果你的 skill 要同时兼容 Claude Code、Codex、openclaw： 1234567891011---name: my-skilldescription: 做某件事。---## 执行步骤运行主脚本：```bashpython scripts/main.py 脚本路径是相对于本 skill 目录（即 SKILL.md 所在目录）的相对路径。 12345678如果你的 skill 专属于 oh-my-opencode，可以用 `@` 语法获得更强的保证：```markdown运行主脚本：```bashuv run @scripts/main.py 5.4 目录结构建议12345678my-skill/├── SKILL.md ← 用相对路径引用，写清楚 `scripts/` 前缀├── scripts/│ └── main.py ← 内部用 __file__ 定位，不假设 cwd├── references/│ └── api-reference.md ← 按需加载的大型参考文档└── assets/ └── template.json ← 静态资源 六、一句话总结 在 SKILL.md 里写 scripts/foo.py（相对路径，无变量），在脚本内部用 __file__ / $(dirname &quot;$0&quot;) 定位自己。这套写法在 Claude Code、Codex、openclaw 三家都能跑，在 oh-my-opencode 里也能跑（或者改用 @scripts/foo.py 获得代码级保证）。 {baseDir} 这个写法仅在 openclaw 的文档里出现，且底层是靠模型推理，不是代码保证。其他三家不认这个变量。 参考 agentskills.io 规范 Codex Skills 官方文档 openai/skills 示例仓库 @anthropic-ai/claude-code v2.1.39 源码（cli.js） @openai/codex v0.104.0 源码（Rust 二进制字符串） oh-my-opencode 源码（src/shared/skill-path-resolver.ts） openclaw + @mariozechner/pi-coding-agent 源码（dist/core/skills.js）","link":"/2026/02/25/agent-skill-path-resolution-across-cli/"},{"title":"把模型烧进晶体管：Taalas HC1 如何用一个&quot;异端&quot;架构击穿 AI 推理成本","text":"最近傅盛写了一篇文章，标题叫《一个疯子，造出便宜 100 倍的 AI 芯片》。 我第一眼看到这个标题，以为是标题党。点进去才发现——这个”疯子”是真实的，这件事也是真实发生的。 这家公司叫 Taalas，来自加拿大多伦多。2026 年 2 月 20 日，他们悄悄从隐身模式退出，发布了第一款产品 HC1。发布前没有发布会，没有 PPT，只有一个可以当场访问的 API 和一个演示用的聊天机器人，叫 ChatJimmy。 速度：17,000 tokens/秒。NVIDIA 最新的 B200：约 350 tokens/秒。 快了将近 50 倍。 这篇文章想做的，不是重复那些性能数字，而是回答一个更本质的问题：他们是怎么做到的？这条路为什么别人没走过？ 一、先理解问题：AI 推理为什么这么贵、这么慢？要理解 Taalas 的创新，必须先理解他们在解决什么问题。 你用 ChatGPT 发一条消息，背后发生了什么？ 模型收到你的问题，开始一个字一个字地生成回答。每生成一个 token（大概 0.75 个英文单词），GPU 都要执行一次完整的矩阵运算。这没什么特别。特别的是数据从哪里来。 一个 8B 参数的模型，权重文件大约 16GB（FP16 精度）。这 16GB 的数据，不是提前放在计算核心旁边等着——它们住在 HBM（高带宽内存）里，跟 GPU 计算单元是两个独立的硬件模块，中间靠总线连接。 每生成一个 token，GPU 就要把整个 16GB 的权重从 HBM 搬运到计算单元，算完结果，再搬回去。下一个 token，再搬一次。 这就是芯片设计里臭名昭著的 Memory Wall（内存墙）：计算速度越来越快，但内存带宽的提升远跟不上，数据搬运成了瓶颈。 根据业界估算，在传统 GPU 推理中，80%–90% 的能量消耗在数据搬运上，真正用于计算的不到 20%。你花了几百万买的 GPU 集群，大部分时间在搬数据，不在算数据。 这不是 NVIDIA 的设计缺陷。这是冯·诺依曼架构的原罪——存储和计算天生分离，1945 年这个架构被提出，80 年来从未有人真正打破过。 大家想了很多办法缓解这个问题： NVIDIA：用 HBM3E，带宽更大，墙更薄 Cerebras：把整张晶圆变成一块芯片，SRAM 离计算核心更近 Groq：堆大量高速 SRAM，减少搬运距离 SambaNova：类似思路 这些都是在优化墙的厚度，但墙还在那里。 Taalas 的做法是：把墙炸掉。 二、核心技术：Mask ROM Recall FabricTaalas 的 CEO Ljubisa Bajic 用了一个很直白的比喻： 传统芯片是每次做饭，都要把整个菜市场搬进厨房，用完再搬走。我们的做法是：把菜直接种在厨房里。 具体怎么实现？ 他们发明了一种叫 Mask ROM Recall Fabric 的架构。 什么是 Mask ROM？ROM（Read-Only Memory，只读存储器）是一种古老的存储技术——数据在芯片制造时就刻进去，之后永久不变。Mask ROM 是其中最彻底的形式：通过光刻掩模（Mask）直接把”1”和”0”的逻辑编码在金属层的连接方式里，不需要电来保持存储，掉电数据也在。 这技术本身不新，老计算器、游戏机卡带里都有。Taalas 的创新在于：把 AI 模型的权重，当成 Mask ROM 的内容烧进去。 一个晶体管 = 存储 + 计算Bajic 透露的核心技巧是： “我们有一套方案，可以在单个晶体管里，既存储 4 bit 的权重，又完成和这个权重相关的乘法运算。一个晶体管全包了。” 这才是真正的突破点。在传统架构里，乘法器（Multiplier）和存储单元是两个分离的电路，数据要”走路”从存储到乘法器。在 HC1 里，数据不走路，因为数据就在乘法器里——或者说，存储本身就是乘法器。 Bajic 形容这是一个”一旦走上这条路，才发现比想象中更好”的方案。他们最初只是想消除内存墙，后来发现密度也远超预期。 HC1 的双层结构HC1 芯片内部不是铁板一块，而是两个层次： 1234567891011┌────────────────────────────────────────┐│ Mask ROM Recall Fabric ││ ——模型权重永久固化，不可修改 ││ ——8B 参数（3bit 量化） ││ ——存储即计算，无数据移动 │├────────────────────────────────────────┤│ SRAM Recall Fabric（可编程部分） ││ ——KV Cache（上下文记忆） ││ ——LoRA 微调权重（可替换） ││ ——适配器和个性化配置 │└────────────────────────────────────────┘ 这意味着 HC1 不是完全死板的。你不能换模型的主体，但你可以： 通过 LoRA 给它注入专业知识（比如医疗、法律领域微调） 调整上下文窗口大小 更换 SRAM 层的适配参数 基础模型固化，个性化能力保留——这是一个很务实的设计权衡。 1970 年代的手工工艺Bajic 提到一个细节让人印象深刻：他们用的是”1970 年代手工晶体管级别的设计方法“——一行一行地手工布局，像雕刻一样。 这不是保守，而是精准。当你知道模型权重是固定的，你就不需要设计通用的可编程逻辑，可以为这一套固定数据做极致的定制化物理排布，让信号流的路径最短、密度最高。现代 EDA 工具的通用性反而是一种浪费。 这种”回到基础”的工程哲学，申请了 14 项专利。 三、数字说话：HC1 和现有方案的差距 系统 推理速度（tokens/s） 每百万 token 成本 功耗 冷却方式 NVIDIA H200 ~230 $0.20–$0.49 ~700W 液冷 NVIDIA B200 ~350 $0.20–$0.49 ~1000W 液冷 Cerebras ~2,000 ~$0.05–$0.10 高 特殊 Groq ~600 — 高 — Taalas HC1 ~17,000 ~$0.0075 200W/卡 普通风冷 几个值得关注的细节： 不需要 HBM。HBM（高带宽内存）是当前 AI 芯片供应链最紧张的环节，三星和 SK Hynix 的 HBM 产能决定了全球 AI 芯片的产量上限。HC1 完全不依赖 HBM，直接绕开了这个卡脖子问题，交货周期可以更短，成本结构也完全不同。 普通风冷。NVIDIA B200 需要液冷机柜，一套下来成本高昂，还对机房基础设施有严格要求。HC1 200W 的功耗，10 张卡的服务器总功耗 2.5kW，普通空气冷却机架就够了。这对中小企业部署是重大利好。 无需批处理。传统 GPU 推理为了提高效率，需要把多个用户的请求打包在一起批量处理，这会引入等待延迟。HC1 速度太快，每个用户的请求可以实时独立响应，低延迟和低成本第一次同时实现了——这在 AI 推理领域是罕见的。 四、最大的限制：一颗芯片，一生只干一件事这里必须说清楚 Taalas 的代价，否则这篇文章就是广告。 HC1 只能运行 Llama 3.1 8B。 换别的模型，这块芯片就废了。 这是架构决定的——权重烧进晶体管，就是物理结构，没有”更新固件”这回事。 对很多场景来说，这是真实的制约。 但 Taalas 有一套缓解方案：结构化 ASIC + 两层金属快速更新。 HC1 的底层逻辑电路是固定的，但模型权重存在芯片最上面的金属互连层（Metal Layers）里。想换一个模型，只需要修改这两层金属的光刻版图，其他层完全复用。 这将”换模型出新芯片”的周期从传统 ASIC 的 6–12 个月，压缩到了 2 个月。定制一款专属推理芯片的成本，大约是训练该模型成本的 1/100。 当然，2 个月依然远比软件更新慢。这条路的可行性，很大程度上取决于一个假设：用户会在一个模型上稳定停留足够长的时间。 从目前的趋势看，这个假设有一定支撑——不少企业用户在工作流跑通之后，并不急着追最新版本。但 AI 模型迭代的速度依然是这个商业模式最大的不确定性。 五、这件事的更大意义Taalas 让我想到的不只是”又一家挑战英伟达的公司”。 这件事本身有两个层次值得关注： 第一层：推理成本正在被打穿，从两个方向同时进行。 DeepSeek 从训练端下手——用更聪明的算法，让模型用更少的算力训练出来，把推理的计算量降下来。Taalas 从硬件端下手——让同样的模型，在专用硬件上跑得极其便宜。两条路，同一个方向：让每一个 AI token 的成本趋近于零。 第二层：25 个人，3000 万美元，打穿了一个几百亿美元的市场的护城河。 Tenstorrent 做通用 AI 芯片，融了 7 亿美元，估值 32 亿。Taalas 放弃通用性，只做一件事做到极致，3000 万美元出了一个跑赢旗舰 GPU 10 倍的产品。资源和效率之间的关系，在这个案例里看得特别清楚。 Bajic 自己说：越聚越窄，越聚越深，越聚越快。 这句话不只是芯片设计的原则，也像是一种工程哲学。 六、现在能用吗？可以。 Taalas 已经开放了两个入口： 演示体验：chatjimmy.ai，直接感受 17,000 tokens/秒是什么速度 API 申请：taalas.com/api-request-form，开发者可以申请接入测试 产品路线图： 时间 里程碑 现在 HC1：Llama 3.1 8B，API 已开放 2026 夏 HC1 扩展版：支持 20B 参数模型 2026 年底 HC2：前沿模型（Llama 4 / DeepSeek），多卡流水线，FP4 精度 12 个月内 千亿参数级前沿 LLM 支持 尾声我习惯对”颠覆 NVIDIA”的新闻保持怀疑。每隔几个月就有一家。 但 Taalas 的这件事让我停下来认真想了一下。不是因为 17,000 这个数字，而是因为那个技术决策的逻辑链条——放弃通用性，否定冯·诺依曼，把模型变成硬件本身——这个思路是自洽的，数字是可以被独立验证的，芯片是真实量产的。 它不会取代 GPU，因为训练永远需要通用算力。但在”大规模稳定推理”这个场景里，它可能真的找到了一条完全不同的路。 结局还没写完。但开头确实很精彩。 参考资料 Taalas 官方博客：The Path to Ubiquitous AI NextPlatform 深度报道：Taalas Etches AI Models Onto Transistors 钛媒体：“邪修”AI芯片的Taalas，成色如何？ 傅盛：一个疯子，造出便宜100倍的AI芯片 Hacker News 讨论：The path to ubiquitous AI (17k tokens/sec)","link":"/2026/02/24/taalas-hc1-ai-chip-deep-dive/"},{"title":"YOLO 目标检测系列算法的发展历程与版本对比","text":"引言在计算机视觉领域，目标检测是一个永恒的核心课题。从早期的滑动窗口+手工特征，到后来的两阶段检测范式（R-CNN系列），再到如今单阶段算法的全面崛起，这个领域经历了翻天覆地的变化。而在单阶段检测算法中，YOLO（You Only Look Once）系列无疑是最具影响力、工程化最成功的代表。 从2016年Joseph Redmon发布YOLOv1至今，YOLO系列已经走过了十个年头，迭代了12个主要版本。这期间，不仅有原作者的天才设计，还有来自工业界和学术界的持续贡献，形成了一个蓬勃发展的生态系统。本文将系统梳理YOLO系列的发展脉络，对比各版本的核心特性与性能，并给出实际场景下的选型建议。 发展历程：从天才idea到工业标准奠基期：YOLOv1（2016）2016年，Joseph Redmon的一篇论文《You Only Look Once: Unified, Real-Time Object Detection》彻底改变了目标检测的格局。在此之前，两阶段算法（如Faster R-CNN）虽然精度较高，但推理速度较慢，难以满足实时需求。 Figure 2 from YOLOv1 (Redmon et al., 2016)：检测编码为 S×S×(B·5+C) 张量，每个格子负责预测中心点落入其中的目标。 Figure 3 from YOLOv1：网络由 24 层卷积层后接 2 层全连接层组成，交替使用 1×1 卷积降维。 核心突破： 首次将目标检测视为回归问题，直接从图像像素回归出边界框坐标和类别概率 单网络端到端训练，无需区域提议（Region Proposal）阶段 将图像划分为S×S网格，每个网格负责预测中心点落在其中的目标 点评：YOLOv1的设计哲学极具颠覆性——“看一次就够了”。这种简洁性带来了极快的推理速度，但也存在明显局限：对小目标和密集目标检测效果较差，边界框定位不够精确。然而，它为后续所有YOLO版本奠定了”单阶段、实时、端到端”的基调。 进化期：YOLOv2/YOLO9000（2017）Redmon很快意识到v1的不足，并在次年推出了YOLOv2，同时还发布了一个野心勃勃的版本——YOLO9000，能检测超过9000类物体。 Figure 3 from YOLOv2/YOLO9000 (Redmon &amp; Farhadi, 2017)：预测宽高相对于聚类中心的偏移，用 sigmoid 将中心坐标限制在当前格子范围内。 核心改进： 引入Anchor Box（借鉴Faster R-CNN），大幅提升边界框召回率 添加Batch Normalization，加速收敛并提升模型稳定性 采用多尺度训练，让模型适应不同分辨率的输入 使用Darknet-19作为骨干网络，平衡精度与速度 YOLO9000：提出了一种联合训练方法，能同时利用检测数据集和分类数据集 点评：YOLOv2是一次非常务实的改进。Anchor Box的引入是关键——它解决了v1中”每个网格只能预测一个目标”的限制。多尺度训练则让模型变得更加鲁棒。YOLO9000的联合训练思路很有想象力，虽然在当时实用性有限，但为后来的开放词汇检测埋下了伏笔。 成熟期：YOLOv3（2018）YOLOv3是Redmon的最后一个主要版本，也是整个系列中最经典、最具生命力的版本之一。即使在今天，仍有大量项目基于v3进行开发。 核心改进： 提出Darknet-53骨干网络，更深的网络结构带来更强的特征提取能力 实现多尺度预测（FPN的雏形），在3个不同尺度的特征图上进行检测，有效解决了小目标检测问题 每个预测点预测3个Anchor Box，进一步提升召回率 用逻辑回归替代softmax进行分类，支持多标签分类 点评：YOLOv3的设计体现了”实用主义”的巅峰。Darknet-53借鉴了ResNet的残差思想，既保证了深度又避免了梯度消失。多尺度预测是另一个关键改进——它让模型能够同时捕捉不同大小的目标，这是v3相比v2最大的提升。Redmon在发布v3后不久便宣布退出计算机视觉领域，但他留下的v3却成为了工业界的”常青树”。 转折期：YOLOv4（2020）Redmon离开后，YOLO系列的火炬传到了Alexey Bochkovskiy手中。2020年，他发布了YOLOv4，这是YOLO系列发展史上的一个重要转折点。 Figure 2 from YOLOv4 (Bochkovskiy et al., 2020)：完整检测器由 Input、Backbone、Neck、Prediction Head 四部分组成。 核心改进： 引入CSP（Cross Stage Partial）结构，优化梯度流动，减少计算量 提出Mosaic数据增强，将4张图片拼接成一张，极大丰富了背景信息 使用PANet（Path Aggregation Network） 替代FPN，增强特征融合能力 采用CIoU Loss，同时考虑边界框的重叠面积、中心点距离和长宽比 加入大量训练技巧：余弦退火学习率、label smoothing等 点评：YOLOv4标志着YOLO系列从”个人作品”向”社区协作”的转变。AlexeyAB没有追求革命性的架构创新，而是将当时各种有效的技巧整合在一起——CSP、Mosaic、PANet、CIoU……这些改进虽然单独来看都不是首创，但组合起来却产生了显著的效果。Mosaic增强尤其聪明，它用一种极其简单的方式大幅提升了模型的鲁棒性。 工程化：YOLOv5（2020）就在YOLOv4发布后不久，Ultralytics公司推出了YOLOv5。这个版本在学术界引发了一些争议（因为没有对应的论文），但在工程界却获得了巨大的成功。 核心改进： 完全迁移至PyTorch框架，告别了Darknet的晦涩代码 提出Focus模块，对特征图进行切片再拼接，减少下采样的信息损失 完善的工程生态：丰富的预训练模型、简单的API、完整的部署工具链 灵活的模型缩放策略：通过深度因子和宽度因子控制模型大小（Nano/Small/Medium/Large） 点评：YOLOv5的最大贡献不在于算法创新，而在于工程化。Ultralytics将YOLO从一个”算法”变成了一个”产品”。PyTorch的迁移让更多开发者能够轻松上手，完善的工具链让部署变得前所未有的简单。v5的成功告诉我们：在工业界，有时好的工程实现比论文中的新点子更重要。 工业化：YOLOv6（2022）2022年，美团视觉智能部发布了YOLOv6，这是国内工业界对YOLO系列的重要贡献。v6的设计目标非常明确——面向端侧部署。 核心改进： 引入RepVGG重参数化技术，训练时使用多分支结构提升精度，推理时合并为单分支提升速度 专为端侧优化的骨干网络设计 提供更丰富的量化支持 点评：YOLOv6代表了YOLO系列在工业化落地方向上的深入探索。重参数化是一个极具实用价值的技术——它让我们能够”鱼与熊掌兼得”：训练时的高精度和推理时的高效率。v6的出现也说明，YOLO系列已经不再是少数人的游戏，而是全球工业界共同参与的平台。 架构创新：YOLOv7（2022）同样是2022年，Chien-Yao Wang等人发布了YOLOv7。这个版本在学术界获得了很高的评价，被认为是”学术派”YOLO的代表。 Figure 2 from YOLOv7 (Wang et al., 2022)：E-ELAN 不改变原有梯度传输路径，用 group conv + cardinality shuffle 增强特征表达能力。 核心改进： 提出E-ELAN（Extended Efficient Layer Aggregation Network），通过控制最短最长梯度路径来增强梯度流动 深入探索模型重参化策略，提出了更科学的重参化方法 使用动态标签分配策略，优化训练过程 点评：YOLOv7是一次扎实的架构创新。E-ELAN的设计体现了对梯度流动的深刻理解——它让网络能够更深、更高效地学习。v7证明了：即使在YOLO这样已经高度优化的架构上，仍然有通过精细设计提升性能的空间。 Anchor-Free：YOLOv8（2023）2023年，Ultralytics带着YOLOv8强势回归。这是Ultralytics继v5之后的又一力作，也是YOLO系列的一次架构跃迁。 核心改进： 解耦头：将分类和检测头分开，让两个任务各自优化 Anchor-Free：抛弃了Anchor Box的设计，直接预测目标的中心点和宽高 任务统一化：在同一框架下支持检测、分割、姿态估计等多种任务 新的骨干网络和Neck设计 点评：YOLOv8是一次勇敢的架构升级。解耦头的设计非常直观——分类和检测本来就是两个不同的任务，用不同的头来处理是合理的。Anchor-Free则进一步简化了检测流程，减少了超参数的数量。更重要的是，v8开始向”多任务统一”的方向迈进，这是计算机视觉的一个重要趋势。 信息可编程：YOLOv9（2024.02）2024年2月，WongKinYiu（v4、v7的作者之一）发布了YOLOv9。这个版本的核心思想是”可编程梯度信息”。 Figure 3 from YOLOv9 (Wang et al., 2024)：PGI 由主分支（推理用）、辅助可逆分支（提供可靠梯度）、多级辅助信息三部分构成。 Figure 4 from YOLOv9：GELAN 将 CSPNet 与 ELAN 结合，可插入任意计算块，在轻量化的同时保持高精度。 核心改进： 提出PGI（Programmable Gradient Information），解决深层网络训练时的信息丢失问题 设计GELAN（Generalized Efficient Layer Aggregation Network），在保持轻量化的同时提升精度 重新思考特征融合策略，确保梯度能有效流动 点评：YOLOv9的设计哲学很有深度——它关注的是”信息如何在网络中流动”。PGI的核心思想是：在深层网络中，梯度信息会逐渐丢失，我们需要一种方式来”编程”这些信息，让它们能够有效地传播。v9在COCO数据集上取得了当时的最高精度，证明了这种思路的有效性。 端到端：YOLOv10（2024.05）2024年5月，清华大学的团队发布了YOLOv10。这个版本的目标很明确：实现真正的”端到端”检测，去掉NMS后处理。 Figure 2 from YOLOv10 (Wang et al., 2024)：训练时同时优化两条分支，推理时只用一对一分支，天然无重复预测，无需 NMS。 核心改进： NMS-Free：通过双标签分配策略，让模型直接输出无冗余的检测结果，无需NMS后处理 优化的模型架构，在保持精度的同时提升效率 更低的延迟，特别适合高密度检测场景 点评：NMS是目标检测流程中一个”令人讨厌”的步骤——它会带来额外的延迟，而且不是可微的，无法在训练中优化。YOLOv10通过巧妙的标签分配策略去掉了NMS，实现了真正的”端到端”。这在高密度场景下特别有价值，因为NMS的延迟往往会随着目标数量的增加而增加。 全能王：YOLOv11（2024.09）2024年9月，Ultralytics发布了YOLOv11。这个版本将”多任务统一”推向了新的高度。 核心改进： 提出C3k2/C2PSA架构，进一步优化特征提取和融合 统一支持**检测、分割、姿态估计、OBB（旋转框）**等多种任务 更完善的量化和部署工具链 更好的小目标检测能力 点评：YOLOv11代表了”大一统”的趋势。在一个框架下支持多种任务，不仅降低了开发成本，也让任务之间的信息能够相互促进。v11的工程生态也是所有YOLO版本中最完善的——从训练到部署，从量化到加速，几乎所有环节都有现成的工具。 注意力时代：YOLOv12（2025.02）2025年2月，Ultralytics发布了最新的YOLOv12。这个版本标志着YOLO系列正式进入”注意力时代”。 Figure 2 from YOLOv12 (Tian et al., 2025)：Area Attention 将特征图等分为 l 个区域（默认4），在区域内做自注意力，兼顾大感受野与线性复杂度。 Figure 3 from YOLOv12：R-ELAN（Residual ELAN）通过残差连接解决深层网络梯度衰减，确保训练稳定性。 核心改进： 引入Area Attention，将自注意力复杂度从O(n²)降至线性，通过划分水平/垂直区域来实现高效的全局建模 设计R-ELAN，在GELAN基础上添加残差连接，改善梯度流动 集成FlashAttention优化，让注意力操作在实时推理中可用 平衡全局建模和局部特征提取，在精度和效率之间取得更好的平衡 点评：YOLOv12是一次大胆的尝试——将Transformer中的自注意力机制引入到实时检测架构中。Area Attention的设计非常巧妙：它既保留了自注意力的全局建模能力，又通过区域划分将复杂度控制在可接受范围内。v12的出现标志着：注意力机制不再只是NLP和大模型的专利，它正在全面进入实时计算机视觉领域。 版本对比：数据说话为了更直观地对比各版本的性能，我们整理了近年来主要版本在COCO val2017数据集上的表现。 精度对比（mAP 50-95） 规格 YOLOv8 YOLOv9 YOLOv10 YOLOv11 YOLOv12 Nano 37.3 38.3 38.5 39.5 38.0 Small 44.9 46.8 46.3 47.0 ~47.5 Medium 50.2 51.4 51.3 51.5 ~52.0 Large 52.9 53.0 53.2 53.4 ~54.0 XLarge 53.9 55.6 54.4 54.7 55.2 从精度数据来看，整体呈现稳步提升的趋势。值得注意的是： YOLOv9-E在XLarge规格下取得了55.6的最高精度，这在当时是非常出色的成绩 YOLOv11在中低规格下表现突出，Nano规格比v8高出2.2个mAP YOLOv12在Small以上规格中表现最好，说明Area Attention在较大模型上更有优势 参数量与效率对比（X规格） 版本 参数量 (M) GFLOPs mAP 50-95 YOLOv8-X 68.2 257.8 53.9 YOLOv9-E 58.1 192.5 55.6 YOLOv10-X 29.5 160.4 54.4 YOLOv11-X 56.9 194.9 54.7 YOLOv12-X 57.5 138.0 55.2 效率数据更有意思： YOLOv10-X的参数量只有29.5M，是所有X规格中最小的，这得益于NMS-Free设计带来的简化 YOLOv12-X的GFLOPs只有138.0，是所有版本中最低的，同时精度却达到了55.2——这充分体现了Area Attention的效率优势 YOLOv9-E在参数量和GFLOPs都低于v8-X的情况下，精度反而更高，说明PGI和GELAN的设计非常高效 重要变体：百花齐放除了主要版本外，YOLO生态中还涌现出了许多重要的变体，它们针对不同的应用场景进行了优化。 YOLO-World（腾讯AILab）核心特点：开放词汇检测，文本驱动，无需重训 点评：YOLO-World是一个非常有想象力的作品。它将YOLO的实时能力与CLIP的开放词汇能力结合起来，让用户能够通过文本描述来检测任意类别的物体，而无需重新训练模型。这种”即插即用”的特性在很多实际场景中非常有用，比如工业质检中的缺陷检测（缺陷类型往往是多样化且难以预定义的）。 YOLO-NAS（Deci AI）核心特点：NAS架构搜索，INT8量化友好 点评：YOLO-NAS代表了”自动化设计”的方向。Deci AI使用神经架构搜索（NAS）技术，自动设计出更高效的架构。更重要的是，他们在搜索过程中就考虑了量化友好性，使得YOLO-NAS在INT8量化下的性能损失非常小。这对于边缘设备部署来说非常有价值。 RT-DETR（百度）核心特点：Transformer实时检测，mAP 53-54.8 点评：RT-DETR是Transformer在实时检测领域的成功尝试。它证明了：只要设计得当，Transformer架构也能达到实时性能。RT-DETR的NMS-Free设计也很有特色——它通过匈牙利算法直接进行一对一匹配，避免了NMS带来的延迟。 Gold-YOLO（华为诺亚）核心特点：Gather-Distribute聚合分发机制 点评：Gold-YOLO提出了一种新的特征融合思路——Gather-Distribute。与传统的FPN/PANet不同，它先将所有尺度的特征聚合到一起，然后再分发给各个尺度。这种”先聚后分”的方式能够更有效地利用跨尺度信息，在小目标检测上表现尤其突出。 场景选型建议说了这么多，到底该选哪个版本呢？我们根据不同的需求给出以下建议： 需求 推荐 原因 嵌入式/边缘设备 YOLOv11-N/S 参数最小，量化工具链成熟 极低延迟（服务器） YOLOv10-S/M NMS-Free，高密度场景延迟优势 生产环境通用 YOLOv11-M 生态最完善，多任务支持 学术/高精度 YOLOv9-E 或 YOLOv12-X 精度最高，架构最前沿 动态类别检测 YOLO-World 开放词汇，文本驱动 具体场景分析1. 嵌入式/边缘设备 如果你的部署目标是嵌入式设备（如树莓派、Jetson Nano、手机等），那么YOLOv11的Nano或Small规格是最佳选择。v11在小模型上的优化非常出色，而且Ultralytics提供了完善的量化和部署工具链，支持ONNX、TensorRT、NCNN等多种推理框架。 2. 极低延迟（服务器） 如果你需要在服务器上运行极低延迟的检测（比如实时视频流分析、自动驾驶的感知系统），那么YOLOv10是更好的选择。v10的NMS-Free设计在高密度场景下特别有优势——随着目标数量的增加，v10的延迟增长比其他版本慢得多。 3. 生产环境通用 对于大多数生产环境的应用场景，YOLOv11-M是最稳妥的选择。它的精度足够高，速度足够快，而且生态最完善——从训练到部署，从数据增强到模型量化，几乎所有环节都有现成的工具。此外，v11的多任务支持（检测、分割、姿态、OBB）也让它能够应对更多样化的需求。 4. 学术/高精度 如果你在做学术研究，或者对精度有极高的要求（比如医疗影像分析、卫星图像解读），那么YOLOv9-E或YOLOv12-X是最佳选择。v9-E在XLarge规格下取得了55.6的mAP，是目前所有YOLO版本中最高的。v12-X则在精度和效率之间取得了更好的平衡，而且它的Area Attention设计也很有研究价值。 5. 动态类别检测 如果你需要检测的类别是动态的、无法预定义的（比如工业质检中的缺陷检测、开放环境下的监控），那么YOLO-World是唯一的选择。它让你能够通过文本描述来定义检测类别，无需重新训练模型——这种灵活性在很多实际场景中是不可替代的。 YOLOv12技术亮点深度解析作为YOLO系列的最新版本，YOLOv12有几个技术亮点值得我们深入探讨。 Area Attention：线性复杂度的全局建模自注意力机制（Self-Attention）的最大问题是复杂度——它的计算量是O(n²)，其中n是token的数量。对于高分辨率图像来说，这是不可接受的。 YOLOv12提出的Area Attention巧妙地解决了这个问题。它的核心思想是： 将特征图划分为水平区域和垂直区域 在每个区域内进行自注意力计算 这样既保留了一定的全局建模能力，又将复杂度从O(n²)降至线性 点评：Area Attention是一种非常”工程化”的创新。它没有追求理论上的完美，而是在精度和效率之间找到了一个很好的平衡点。通过区域划分，它既避免了全局注意力的高昂计算成本，又比纯粹的局部注意力拥有更大的感受野。 R-ELAN：改善梯度流动YOLOv12在GELAN（来自YOLOv9）的基础上，添加了残差连接，形成了R-ELAN。 设计思路： 保留GELAN的高效特征聚合能力 添加残差连接，确保梯度能够直接传播到较早的层 这种设计让网络能够更深，同时避免梯度消失 点评：残差连接（Residual Connection）并不是什么新东西，但它的有效性已经被无数次证明。YOLOv12将它和GELAN结合起来，是一次”简单但有效”的改进。这也说明：在架构设计中，有时不需要追求花哨的新东西，把已有的技巧用好就足够了。 FlashAttention：让注意力在实时推理中可用FlashAttention是近年来最重要的深度学习优化之一。它通过重新组织计算顺序，大幅提升了自注意力的计算速度和内存效率。 YOLOv12集成了FlashAttention优化，这使得Area Attention在实时推理中变得可用。没有FlashAttention，即使Area Attention的理论复杂度很低，实际运行速度可能也无法接受。 点评：好的算法需要好的实现才能发挥价值。FlashAttention就是一个典型的例子——它不是改变算法本身，而是改变算法的实现方式，却带来了巨大的性能提升。YOLOv12的作者们显然很清楚这一点，他们及时地将最新的优化技巧整合到了架构中。 结语：YOLO系列的启示回顾YOLO系列十年的发展历程，我们可以得到很多启示： 1. 简洁性是工程成功的关键 YOLOv1的成功，很大程度上在于它的简洁——“看一次就够了”。这种简洁性让它容易理解、容易实现、容易优化。在工程领域，简单的方案往往比复杂的方案更有生命力。 2. 架构创新和工程化同样重要 YOLOv3、v7、v9、v12代表了架构创新的方向，而v5、v6、v8、v11则代表了工程化的方向。这两条线都很重要——没有架构创新，就没有性能的突破；没有工程化，再好的算法也难以落地。 3. 社区协作是技术进步的加速器 Redmon在的时候，YOLO是个人作品；Redmon离开后，YOLO变成了社区协作的平台。AlexeyAB、Ultralytics、美团、清华大学……来自全球的开发者和研究者共同推动着YOLO系列的发展。这种社区协作模式大大加速了技术进步。 4. 趋势总是会到来的 从两阶段到单阶段，从Anchor-Based到Anchor-Free，从CNN到Attention……这些趋势在早期都充满争议，但最终都成为了主流。YOLO系列的发展历程告诉我们：不要抗拒趋势，要主动拥抱趋势，在合适的时候将新的技术整合到自己的架构中。 未来，YOLO系列会走向何方？我们可以期待：更好的多任务统一，更强的开放词汇能力，更高效的注意力机制，更完善的工具链……但有一点是肯定的：YOLO系列还会继续发展，继续推动计算机视觉的进步。 对于我们这些使用者来说，最重要的不是追最新的版本，而是根据自己的需求选择合适的版本，把它用好。毕竟，在实际项目中，把一个模型用好比用一个”最好”的模型更重要。","link":"/2026/02/25/yolo-series-development-and-comparison/"},{"title":"强者 Vibe，弱者 Wipe","text":"写在前面2025 年 2 月 6 日，Andrej Karpathy 在 X 上发了一条推文，用了一个新词：Vibe Coding。 他这样描述这种体验： “完全交给感觉（Vibes），拥抱指数增长，甚至忘记代码的存在。” 他用 SuperWhisper 语音输入指令，让 Cursor 帮他改 UI 边距、调逻辑，根本不看生成的代码——“it just works, and I don’t fully understand why”。 一石激起千层浪。这条推文在技术圈引发了一场至今未平息的争论，争论的核心不是技术本身，而是一个更根本的问题：编程这件事，究竟是什么？ 一、布道者眼中的 Vibe Coding：意图平权的曙光硅谷的创业者和独立开发者群体是 Vibe Coding 最热情的拥趸。 他们的核心叙事是：软件 3.0 时代到来了。编程的重心正在从”语法实现”转向”意图表达”——你不再需要记住 Array.prototype.reduce 的用法，你只需要知道你想干什么。 典型故事是这样的：一个人，一个周末，40 小时，从零上线一个功能完整的 SaaS。没有后端工程师，没有 DevOps，甚至没有太多编程经验。他们管这个叫”消除想法与产品之间的摩擦”。 YC 的某些批次已经出现了整个公司只有一两个人、但代码库有几万行的情况，背后全靠 AI 代理撑着。 这个叙事有其迷人之处。它意味着：那些被传统编程门槛挡在门外的人——设计师、产品经理、领域专家——现在可以直接把脑子里的东西变成软件。这不是小事，这是创造力的民主化。 二、资深工程师眼中的 Vibe Coding：定时炸弹同样一件事，在有十年以上经验的工程师眼里，画风就完全不同了。 他们的批评集中在几个点： 代码质量：AI 生成的代码往往是”面条代码”，能跑但没有结构，没有对未来维护的考量。重构的时候你才会发现，这堆东西根本不是在解决问题，而是在绕开问题。 安全盲区：Snyk 等安全机构的研究发现，Vibe Coding 特别容易引入安全漏洞——因为开发者根本不看代码逻辑，只看有没有报错。一个真实的案例：某 SaaS 创业者用 Vibe Coding 快速上线，因为 AI 悄悄跳过了认证逻辑，用户数据被轻易拖走了。 “代码清洁工”困境：流传最广的一个说法是，AI 让初级程序员变成了提示词工程师，却让资深程序员变成了”Code Janitors”——整天在收拾 AI 留下的烂摊子，找那些隐蔽的、只有经验才能识别的 Bug。 这批人并不是在反对 AI 辅助编程本身，他们用 Copilot、Cursor，也觉得很好用。他们反对的是”忘记代码存在”这个姿态——不理解就不能拥有，这是他们的核心信念。 三、初学者眼中的 Vibe Coding：学习方式的重构对于正在学编程的人来说，这件事更复杂，充满了矛盾感。 一部分初学者发现，Vibe Coding 是个神奇的学习工具。与其看视频教程被动接收，不如直接告诉 AI “我想做一个登录页面”，然后观察代码是怎么变化的，遇到不懂的直接问。这种主动探索的方式，对某些人来说比传统教学有效得多。 但另一部分人，尤其是计算机专业的学生，陷入了一种新的焦虑：我到底在学什么？ 如果 AI 能帮你写所有的代码，那学数据结构、学算法、啃操作系统原理的意义是什么？如果你只会调用 AI，你是程序员吗？未来市场上要的是什么人？ 这种焦虑是真实的，也是合理的。教育体系还没有给出一个清晰的答案。 四、技术领袖眼中的 Vibe Coding：各说各话Karpathy 本人后来有所补充。他说他其实并不主张所有人都这样做，Vibe Coding 对他有效，是因为他有足够的底子，知道 AI 在说谎的时候能识别出来。到 2026 年初，他又提出了新词”Agentic Engineering”，强调 AI 不再只是辅助写一行代码，而是作为自主代理负责整个系统设计——这是 Vibe Coding 的进化版，但也需要更强的工程判断力。 Sam Altman 的表态颇为微妙。他亲身试验后感叹，这个过程很有趣，但让他有一种”无用感”（feeling a little useless）——AI 给的方案比他自己想的更好更快。他对学生的建议是学会掌握 AI 工具，而不是死磕特定语言。 而大多数 CTO 保持谨慎。一项访谈 18 名 CTO 的研究发现，其中 16 人报告过因过度依赖 AI 生成代码导致的生产事故。他们倾向于把 AI 用在测试和文档上，而不是核心业务逻辑。 五、数据说了什么有几个数字值得关注： 截至 2025 年底，约 41% 的代码由 AI 辅助或自动生成。 重度 AI 用户的自我感知生产力提升了约 3 倍。 但 METR 2025 年做的一项对照实验显示：使用 AI 的开发者自认为快了 20%，实际测量却比不用 AI 的慢了 19%。 最后那个数据最刺眼。它揭示了一个可能正在大规模发生的认知偏差：我们感觉在飞，实际上可能在原地打转。原因或许是：AI 生成代码快，但理解、调试、验证这些代码花的时间被低估了；再加上错误方向上的快速推进，代价更大。 六、我的思考我不认为 Vibe Coding 是一个需要站队的问题。它不是”AI 让编程变好了”或”AI 毁了编程”这种二元叙事能概括的。 我更感兴趣的是它暴露出来的一个深层矛盾： 我们对”理解”的要求，和我们对”产出”的要求，正在发生分裂。 传统编程里，理解和产出是绑定的——你写出来的代码，就是你理解的具象化。Vibe Coding 打破了这个绑定：你可以有产出，但不必有理解。 这不是第一次发生类似的事。框架出现的时候，有人说”用了 Spring 的 Java 程序员不算真正的 Java 程序员”；用 Stack Overflow 的时候，有人说”复制粘贴的不是真正的工程师”。每一次，这种担忧都有一定道理，但历史最终走向了接受，因为抽象层的提升解放了更大的创造力。 但这一次有些不同。之前的每一层抽象，都还要求你理解你在干什么；你得看懂 Stack Overflow 的答案，你得理解框架的边界。Vibe Coding 在某种极端形式下，连这个最低要求也放弃了。 这不是抽象，这是外包。 外包没有问题，前提是你知道外包出去的东西后来会发生什么。一个有经验的工程师做 Vibe Coding，他的”感觉”（Vibe）背后有十年的工程直觉在兜底，他知道什么时候该停下来看一眼代码，知道哪里会出问题。一个没有这个底子的人做 Vibe Coding，他的”感觉”就是真的只是感觉——到出问题的那一天，他既不知道哪里错了，也不知道怎么修。 所以我的结论不是”Vibe Coding 好”或”Vibe Coding 坏”，而是：Vibe Coding 是一个放大器，它放大你已经拥有的东西。 有底子的人，它是加速器。没底子的人，它是一个让人感觉良好的幻觉制造机。 本身强才能 Vibe，本身弱 Vibe 会毁了你。 这让我想到一个更有意思的问题：在 AI 无处不在的时代，”底子”的定义是不是也在改变？如果连 Karpathy 都说”忘记代码存在”，那未来的工程师底子应该是什么？ 也许不再是”能写出来”，而是**”知道什么是对的”**——对系统、对安全、对用户的判断力。这种判断力，目前还没有捷径。 写在最后Vibe Coding 这个词之所以有趣，是因为它诚实。 它没有说”AI 辅助编程”或”智能代码生成”，它直接说：这就是靠感觉。这种诚实反而让围绕它的讨论更真实——它逼着每个人说清楚自己到底在乎什么，在乎代码本身，在乎产品结果，还是在乎某种职业认同。 不同人眼中的 Vibe Coding，照出来的其实是不同人眼中的”编程是什么”。 而这个问题，可能比 Vibe Coding 本身更值得想清楚。","link":"/2026/03/01/vibe-coding-perspectives/"},{"title":"Agent Client Protocol（ACP）：AI 编码世界的 LSP","text":"写在前面如果你用过 VS Code + Copilot、Zed + Claude、Cursor 等工具，你可能已经注意到一个现象：每个 AI Coding Agent 都需要跟编辑器深度集成，而每次集成都是各搞各的，互不兼容。 换一个 Agent，就得在编辑器里重新配置一遍；换一个编辑器，原来的 Agent 不一定能用。 Agent Client Protocol（ACP） 就是为了解决这个问题诞生的。 一、它是什么？ACP（Agent Client Protocol）是一个标准化 AI Coding Agent 与代码编辑器（IDE）之间通信的开放协议。 类比来说： ACP 之于 AI Agent，就像 LSP（Language Server Protocol） 之于语言工具。 LSP 出现之前，每个编辑器都要为每种编程语言单独实现代码补全、跳转、诊断。LSP 出现之后，语言服务器只需实现一次，任何支持 LSP 的编辑器都可以直接用。 ACP 要解决的是同样的 N×M 问题： 二、整体架构两个角色：Client 和 AgentACP 定义了两个核心角色： Client（客户端）：通常是代码编辑器（Zed、VSCode、JetBrains 等），也可以是其他 UI。负责： 管理用户交互 控制文件系统访问 提供终端能力 渲染 Agent 的输出 Agent（代理）：AI 编码代理（Claude、Copilot、Cline、Kimi Code 等），负责： 接收用户 prompt 调用 LLM 推理 执行工具调用（读写文件、运行命令） 流式返回结果 通信方式目前 ACP 支持两种传输： 场景 传输方式 本地 Agent JSON-RPC over stdio（Agent 作为编辑器子进程启动） 远程 Agent HTTP / WebSocket（草案阶段，尚未正式支持） 本地场景下，编辑器直接 spawn 一个 Agent 子进程，双方通过标准输入输出通信。这个设计极其简单，几乎任何语言都能实现。 Session 模型一个连接可以承载多个并发 Session，每个 Session 有自己独立的对话上下文。用户可以同时开多条”思路线”而互不干扰。 三、一次完整的交互流程下面用时序图描述一次典型的 prompt 交互： 整个流程用 JSON-RPC 2.0 编码，双向通信： Methods：有 request/response 的调用（如 initialize、session/prompt） Notifications：单向推送，不需要响应（如 session/update，用于流式输出） Agent 通过密集的 session/update notification 实现实时流式 UI 更新，用户能看到 Agent 在逐步思考、执行。 四、协议的核心特性4.1 双向能力暴露ACP 不是单向的”Client 问 Agent 答”，而是真正双向的： Agent 可以调用 Client 提供的方法： 方法 作用 fs/read_text_file 读取文件内容 fs/write_text_file 写入文件内容 terminal/create 创建终端 terminal/output 获取终端输出 session/request_permission 执行工具前请求用户授权 这意味着 Agent 可以主动读写用户的代码文件、运行命令，而不需要编辑器预先把所有内容塞给它。 4.2 原生 Diff 支持ACP 内置了专为代码变更设计的 diff 内容类型： 123456{ &quot;type&quot;: &quot;diff&quot;, &quot;path&quot;: &quot;/project/src/main.py&quot;, &quot;oldText&quot;: &quot;def hello():\\n print('hi')&quot;, &quot;newText&quot;: &quot;def hello(name: str):\\n print(f'hi {name}')&quot;} 编辑器收到这个消息后，可以直接渲染成漂亮的 diff 视图，让用户清楚看到 Agent 改了什么，再决定是否接受。 4.3 工具调用权限控制Agent 执行工具（写文件、运行脚本）前，必须向用户请求授权： 1234567891011{ &quot;method&quot;: &quot;session/request_permission&quot;, &quot;params&quot;: { &quot;toolCall&quot;: { &quot;title&quot;: &quot;修改 config.py&quot; }, &quot;options&quot;: [ { &quot;optionId&quot;: &quot;allow-once&quot;, &quot;name&quot;: &quot;允许（仅此次）&quot;, &quot;kind&quot;: &quot;allow_once&quot; }, { &quot;optionId&quot;: &quot;allow-always&quot;, &quot;name&quot;: &quot;始终允许&quot;, &quot;kind&quot;: &quot;allow_always&quot; }, { &quot;optionId&quot;: &quot;reject&quot;, &quot;name&quot;: &quot;拒绝&quot;, &quot;kind&quot;: &quot;reject_once&quot; } ] }} 用户可以精细控制 Agent 的操作权限，不用担心 Agent “悄悄”改了东西。 4.4 Agent PlanAgent 可以在开始执行前发布一个执行计划： 123456789{ &quot;sessionUpdate&quot;: &quot;plan&quot;, &quot;entries&quot;: [ { &quot;content&quot;: &quot;读取当前实现&quot;, &quot;status&quot;: &quot;pending&quot; }, { &quot;content&quot;: &quot;分析性能瓶颈&quot;, &quot;status&quot;: &quot;pending&quot; }, { &quot;content&quot;: &quot;重写核心函数&quot;, &quot;status&quot;: &quot;pending&quot; }, { &quot;content&quot;: &quot;运行测试确认&quot;, &quot;status&quot;: &quot;pending&quot; } ]} 编辑器可以把这个计划实时渲染给用户，每个步骤完成后更新状态，让用户清楚知道 Agent 在干什么。 4.5 与 MCP 的集成ACP 和 MCP（Model Context Protocol）是互补的： MCP：解决 Agent 如何访问外部工具和数据源（数据库、API、文件系统等） ACP：解决 Agent 如何与编辑器交互 编辑器在创建 Session 时，可以把用户配置的 MCP Server 信息传给 Agent，Agent 直接连接这些 MCP Server 获取工具能力： 五、当前生态ACP 由 Zed 编辑器团队发起，目前已经有相当规模的生态： 支持 ACP 的 Agent（部分）： Agent 说明 GitHub Copilot 公开预览阶段 Claude Agent 通过 Zed 的 SDK adapter Gemini CLI Google 官方 Kimi CLI 月之暗面 Cline 开源，社区热门 OpenHands 开源，多步骤 Agent Goose Block（Square 母公司）出品 Qwen Code 阿里通义千问 OpenCode SST 出品 支持 ACP 的 Client（部分）： 编辑器：Zed、VSCode（插件）、JetBrains、Neovim（多个插件）、Emacs 其他：Obsidian、marimo notebook、Chrome 扩展、iOS App 六、优点与局限优点1. 打破 N×M 集成困境一套协议打通所有 Agent × Editor 组合，开发者可以自由选工具。 2. UX 原生设计内置 diff、tool call 状态、agent plan、slash commands，天然适配编码场景，而不是把聊天界面硬套进去。 3. 双向通信Agent 可以主动请求 Client 能力，不只是被动回答问题。 4. 与 MCP 兼容复用 MCP 的类型定义，跟整个 AI 工具生态共生共长。 5. 生态已初具规模30+ Agents、30+ Clients，覆盖主流工具。 局限1. 远程 Agent 支持尚未完成HTTP/WebSocket 传输还是草案，目前主要靠 stdio（本地子进程），云端 Agent 接入受限。 2. 聚焦编码场景专为 IDE + Coding Agent 设计，通用 AI Agent 应用无法直接套用。 3. 信任模型较简单文档明确说”ACP 工作在你信任该 Agent 的前提下”，缺乏精细的权限沙箱。 4. 与 MCP 边界模糊两者都在解决 AI 与环境集成的问题，有一定概念重叠，社区存在混淆。 5. 协议尚年轻RFD（Request for Dialog，类似 RFC）列表中有大量未完成提案，核心功能仍在演进。 七、与相关协议对比 ACP MCP LSP 解决问题 Agent ↔ 编辑器通信 Agent ↔ 工具/数据源 语言服务 ↔ 编辑器 通信方向 双向（编辑器也可被 Agent 调用） 主要单向 双向 传输 stdio 为主 stdio / HTTP stdio / TCP 适用场景 Coding Agent UX 工具调用/资源访问 代码补全/跳转/诊断 成熟度 较新，活跃演进 较成熟（Anthropic 主导） 非常成熟（微软主导） 总结ACP 想做的事很清晰：让任意 AI Coding Agent 在任意编辑器里开箱即用，就像 LSP 让任意语言在任意编辑器里开箱即用一样。 协议本身设计得很务实——用成熟的 JSON-RPC 2.0，复用 MCP 的数据类型，把编码 UX 所需的原语（diff、permission、plan、tool call）都内置进来。 生态已经跑起来了，GitHub Copilot、Claude、Gemini、Kimi 这些主流 Agent 都在接入。对于开发者来说，现在了解 ACP 正是时候：它不是遥远的未来标准，而是正在成为现实的基础设施。 参考资料 ACP 官方文档 ACP 架构说明 ACP 协议概览 LSP 协议（对比参考） MCP 协议（对比参考）","link":"/2026/02/26/agent-client-protocol-introduction/"},{"title":"","text":"2026 02 24 开工大吉开工大吉","link":"/2026/02/24/start-working/"}],"tags":[{"name":"大模型","slug":"大模型","link":"/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"},{"name":"春节","slug":"春节","link":"/tags/%E6%98%A5%E8%8A%82/"},{"name":"微信公众号","slug":"微信公众号","link":"/tags/%E5%BE%AE%E4%BF%A1%E5%85%AC%E4%BC%97%E5%8F%B7/"},{"name":"DeepSeek","slug":"DeepSeek","link":"/tags/DeepSeek/"},{"name":"腾讯","slug":"腾讯","link":"/tags/%E8%85%BE%E8%AE%AF/"},{"name":"阿里","slug":"阿里","link":"/tags/%E9%98%BF%E9%87%8C/"},{"name":"OpenClaw","slug":"OpenClaw","link":"/tags/OpenClaw/"},{"name":"企业微信","slug":"企业微信","link":"/tags/%E4%BC%81%E4%B8%9A%E5%BE%AE%E4%BF%A1/"},{"name":"WeCom","slug":"WeCom","link":"/tags/WeCom/"},{"name":"插件","slug":"插件","link":"/tags/%E6%8F%92%E4%BB%B6/"},{"name":"教程","slug":"教程","link":"/tags/%E6%95%99%E7%A8%8B/"},{"name":"Agent","slug":"Agent","link":"/tags/Agent/"},{"name":"Skill","slug":"Skill","link":"/tags/Skill/"},{"name":"Claude Code","slug":"Claude-Code","link":"/tags/Claude-Code/"},{"name":"Codex","slug":"Codex","link":"/tags/Codex/"},{"name":"oh-my-opencode","slug":"oh-my-opencode","link":"/tags/oh-my-opencode/"},{"name":"openclaw","slug":"openclaw","link":"/tags/openclaw/"},{"name":"AgentSkills","slug":"AgentSkills","link":"/tags/AgentSkills/"},{"name":"AI芯片","slug":"AI芯片","link":"/tags/AI%E8%8A%AF%E7%89%87/"},{"name":"推理","slug":"推理","link":"/tags/%E6%8E%A8%E7%90%86/"},{"name":"硬件","slug":"硬件","link":"/tags/%E7%A1%AC%E4%BB%B6/"},{"name":"Taalas","slug":"Taalas","link":"/tags/Taalas/"},{"name":"NVIDIA","slug":"NVIDIA","link":"/tags/NVIDIA/"},{"name":"YOLO","slug":"YOLO","link":"/tags/YOLO/"},{"name":"目标检测","slug":"目标检测","link":"/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"},{"name":"计算机视觉","slug":"计算机视觉","link":"/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"},{"name":"Vibe Coding","slug":"Vibe-Coding","link":"/tags/Vibe-Coding/"},{"name":"AI编程","slug":"AI编程","link":"/tags/AI%E7%BC%96%E7%A8%8B/"},{"name":"思考","slug":"思考","link":"/tags/%E6%80%9D%E8%80%83/"},{"name":"LLM","slug":"LLM","link":"/tags/LLM/"},{"name":"ACP","slug":"ACP","link":"/tags/ACP/"},{"name":"Protocol","slug":"Protocol","link":"/tags/Protocol/"},{"name":"MCP","slug":"MCP","link":"/tags/MCP/"},{"name":"IDE","slug":"IDE","link":"/tags/IDE/"},{"name":"工作","slug":"工作","link":"/tags/%E5%B7%A5%E4%BD%9C/"}],"categories":[{"name":"AI","slug":"AI","link":"/categories/AI/"},{"name":"深度学习","slug":"深度学习","link":"/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"工作","slug":"工作","link":"/categories/%E5%B7%A5%E4%BD%9C/"}],"pages":[{"title":"","text":"SOURCE DIRECTORYAll user-facing content lives here. Hexo reads this directory to generate the site. STRUCTURE12345source/├── _posts/ # Blog posts — the primary authoring target├── about/│ └── index.md # About page (static page, not a post)└── img/ # Static assets: avatar (conan-avatar.svg), logo (logo.svg) WHERE TO ADD THINGS What Where Notes New blog post _posts/YYYY-MM-DD-slug.md slug must be English/ASCII New static page about/, contact/, etc. as index.md use page scaffold Images referenced in posts img/ reference as /img/filename.ext Draft (not published) _drafts/ (create if missing) visible only with npm run dev POST FILENAME CONVENTION Format: YYYY-MM-DD-english-slug.md Slug: lowercase, hyphens only, ASCII — no Chinese characters Example: 2026-03-15-deep-learning-notes.md Hexo derives the permalink from the date + slug; wrong format breaks URLs FRONT MATTER (required fields)1234567title: 中文标题或英文标题date: 2026-03-15 10:00:00categories: - 分类 # ONE category onlytags: - 标签A - 标签B ANTI-PATTERNS No Chinese in filename — breaks URL generation No multiple categories — single category per post only Don’t put assets in public/ — it’s generated output, gitignored, ephemeral Don’t create pages outside source/ root — Hexo only renders from here","link":"/AGENTS.html"},{"title":"","text":"/* ===================================================== Custom overrides — wider content area ===================================================== */ /* Widescreen (≥1216px): unlock the fixed 960px container */ @media screen and (min-width: 1216px) { .is-2-column .container, .is-1-column .container { max-width: calc(100% - 80px) !important; width: calc(100% - 80px) !important; } } /* FullHD (≥1408px): allow up to 1600px before capping */ @media screen and (min-width: 1408px) { .is-2-column .container { max-width: 1520px !important; width: auto !important; } .is-1-column .container { max-width: 960px !important; width: auto !important; } }","link":"/css/custom.css"},{"title":"关于我","text":"👋 你好，我是 luzhongqiu 寻龙分金看缠山，一重缠是一重关 欢迎来到我的个人博客！这里是我记录技术成长、分享学习心得的地方。 🚀 关于我 💻 热爱技术的开发者 🤖 对机器学习和深度学习有浓厚兴趣 📷 喜欢摄影和后期处理 📝 相信知识分享的力量 🛠️ 技术栈 编程语言: Python, Go, JavaScript 机器学习: TensorFlow, PyTorch, Scikit-learn Web 开发: Hexo, Node.js 工具: Git, Docker, Photoshop 📚 博客内容这个博客主要记录： 机器学习与深度学习的实践经验 编程技术与最佳实践 技术问题排查与解决方案 个人成长与思考 📮 联系方式 GitHub: @luzhongqiu 邮箱: 欢迎交流 🙏 致谢感谢你的访问！如果我的博客对你有所帮助，欢迎 Star ⭐ 或分享给你的朋友。 本站使用 Hexo 构建，主题采用 Maupassant","link":"/about/index.html"}]}