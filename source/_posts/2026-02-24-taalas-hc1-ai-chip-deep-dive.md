---
title: 把模型烧进晶体管：Taalas HC1 如何用一个"异端"架构击穿 AI 推理成本
date: 2026-02-24 22:00:00
categories:
  - AI
tags:
  - AI芯片
  - 推理
  - 硬件
  - Taalas
  - NVIDIA
---

最近傅盛写了一篇文章，标题叫《一个疯子，造出便宜 100 倍的 AI 芯片》。

我第一眼看到这个标题，以为是标题党。点进去才发现——这个"疯子"是真实的，这件事也是真实发生的。

这家公司叫 **Taalas**，来自加拿大多伦多。2026 年 2 月 20 日，他们悄悄从隐身模式退出，发布了第一款产品 HC1。发布前没有发布会，没有 PPT，只有一个可以当场访问的 API 和一个演示用的聊天机器人，叫 [ChatJimmy](https://chatjimmy.ai)。

速度：**17,000 tokens/秒**。NVIDIA 最新的 B200：约 350 tokens/秒。

快了将近 **50 倍**。

这篇文章想做的，不是重复那些性能数字，而是回答一个更本质的问题：**他们是怎么做到的？这条路为什么别人没走过？**

---

## 一、先理解问题：AI 推理为什么这么贵、这么慢？

要理解 Taalas 的创新，必须先理解他们在解决什么问题。

你用 ChatGPT 发一条消息，背后发生了什么？

模型收到你的问题，开始一个字一个字地生成回答。每生成一个 token（大概 0.75 个英文单词），GPU 都要执行一次完整的矩阵运算。这没什么特别。特别的是**数据从哪里来**。

一个 8B 参数的模型，权重文件大约 16GB（FP16 精度）。这 16GB 的数据，不是提前放在计算核心旁边等着——它们住在 HBM（高带宽内存）里，跟 GPU 计算单元是两个独立的硬件模块，中间靠总线连接。

每生成一个 token，GPU 就要**把整个 16GB 的权重从 HBM 搬运到计算单元，算完结果，再搬回去**。下一个 token，再搬一次。

这就是芯片设计里臭名昭著的 **Memory Wall（内存墙）**：计算速度越来越快，但内存带宽的提升远跟不上，数据搬运成了瓶颈。

根据业界估算，在传统 GPU 推理中，**80%–90% 的能量消耗在数据搬运上，真正用于计算的不到 20%**。你花了几百万买的 GPU 集群，大部分时间在搬数据，不在算数据。

这不是 NVIDIA 的设计缺陷。这是冯·诺依曼架构的原罪——**存储和计算天生分离**，1945 年这个架构被提出，80 年来从未有人真正打破过。

大家想了很多办法缓解这个问题：
- NVIDIA：用 HBM3E，带宽更大，墙更薄
- Cerebras：把整张晶圆变成一块芯片，SRAM 离计算核心更近
- Groq：堆大量高速 SRAM，减少搬运距离
- SambaNova：类似思路

这些都是在**优化墙的厚度**，但墙还在那里。

Taalas 的做法是：**把墙炸掉**。

---

## 二、核心技术：Mask ROM Recall Fabric

Taalas 的 CEO Ljubisa Bajic 用了一个很直白的比喻：

> 传统芯片是每次做饭，都要把整个菜市场搬进厨房，用完再搬走。我们的做法是：**把菜直接种在厨房里**。

具体怎么实现？

他们发明了一种叫 **Mask ROM Recall Fabric** 的架构。

### 什么是 Mask ROM？

ROM（Read-Only Memory，只读存储器）是一种古老的存储技术——数据在芯片制造时就刻进去，之后永久不变。Mask ROM 是其中最彻底的形式：通过光刻掩模（Mask）直接把"1"和"0"的逻辑编码在金属层的连接方式里，不需要电来保持存储，掉电数据也在。

这技术本身不新，老计算器、游戏机卡带里都有。Taalas 的创新在于：**把 AI 模型的权重，当成 Mask ROM 的内容烧进去**。

### 一个晶体管 = 存储 + 计算

Bajic 透露的核心技巧是：

> "我们有一套方案，可以在单个晶体管里，既存储 4 bit 的权重，又完成和这个权重相关的乘法运算。一个晶体管全包了。"

这才是真正的突破点。在传统架构里，乘法器（Multiplier）和存储单元是两个分离的电路，数据要"走路"从存储到乘法器。在 HC1 里，**数据不走路，因为数据就在乘法器里**——或者说，存储本身就是乘法器。

Bajic 形容这是一个"一旦走上这条路，才发现比想象中更好"的方案。他们最初只是想消除内存墙，后来发现密度也远超预期。

### HC1 的双层结构

HC1 芯片内部不是铁板一块，而是两个层次：

```
┌────────────────────────────────────────┐
│  Mask ROM Recall Fabric                │
│  ——模型权重永久固化，不可修改          │
│  ——8B 参数（3bit 量化）               │
│  ——存储即计算，无数据移动              │
├────────────────────────────────────────┤
│  SRAM Recall Fabric（可编程部分）      │
│  ——KV Cache（上下文记忆）             │
│  ——LoRA 微调权重（可替换）            │
│  ——适配器和个性化配置                 │
└────────────────────────────────────────┘
```

这意味着 HC1 不是完全死板的。你不能换模型的主体，但你可以：
- 通过 LoRA 给它注入专业知识（比如医疗、法律领域微调）
- 调整上下文窗口大小
- 更换 SRAM 层的适配参数

基础模型固化，个性化能力保留——这是一个很务实的设计权衡。

### 1970 年代的手工工艺

Bajic 提到一个细节让人印象深刻：他们用的是"**1970 年代手工晶体管级别的设计方法**"——一行一行地手工布局，像雕刻一样。

这不是保守，而是精准。当你知道模型权重是固定的，你就不需要设计通用的可编程逻辑，可以为这一套固定数据做极致的定制化物理排布，让信号流的路径最短、密度最高。现代 EDA 工具的通用性反而是一种浪费。

这种"回到基础"的工程哲学，申请了 14 项专利。

---

## 三、数字说话：HC1 和现有方案的差距

| 系统 | 推理速度（tokens/s） | 每百万 token 成本 | 功耗 | 冷却方式 |
|------|---------------------|-------------------|------|----------|
| NVIDIA H200 | ~230 | $0.20–$0.49 | ~700W | 液冷 |
| NVIDIA B200 | ~350 | $0.20–$0.49 | ~1000W | 液冷 |
| Cerebras | ~2,000 | ~$0.05–$0.10 | 高 | 特殊 |
| Groq | ~600 | — | 高 | — |
| **Taalas HC1** | **~17,000** | **~$0.0075** | **200W/卡** | **普通风冷** |

几个值得关注的细节：

**不需要 HBM**。HBM（高带宽内存）是当前 AI 芯片供应链最紧张的环节，三星和 SK Hynix 的 HBM 产能决定了全球 AI 芯片的产量上限。HC1 完全不依赖 HBM，直接绕开了这个卡脖子问题，交货周期可以更短，成本结构也完全不同。

**普通风冷**。NVIDIA B200 需要液冷机柜，一套下来成本高昂，还对机房基础设施有严格要求。HC1 200W 的功耗，10 张卡的服务器总功耗 2.5kW，普通空气冷却机架就够了。这对中小企业部署是重大利好。

**无需批处理**。传统 GPU 推理为了提高效率，需要把多个用户的请求打包在一起批量处理，这会引入等待延迟。HC1 速度太快，**每个用户的请求可以实时独立响应**，低延迟和低成本第一次同时实现了——这在 AI 推理领域是罕见的。

---

## 四、最大的限制：一颗芯片，一生只干一件事

这里必须说清楚 Taalas 的代价，否则这篇文章就是广告。

**HC1 只能运行 Llama 3.1 8B。** 换别的模型，这块芯片就废了。

这是架构决定的——权重烧进晶体管，就是物理结构，没有"更新固件"这回事。

对很多场景来说，这是真实的制约。

但 Taalas 有一套缓解方案：**结构化 ASIC + 两层金属快速更新**。

HC1 的底层逻辑电路是固定的，但模型权重存在芯片最上面的金属互连层（Metal Layers）里。想换一个模型，只需要修改这两层金属的光刻版图，其他层完全复用。

这将"换模型出新芯片"的周期从传统 ASIC 的 6–12 个月，压缩到了 **2 个月**。定制一款专属推理芯片的成本，大约是训练该模型成本的 **1/100**。

当然，2 个月依然远比软件更新慢。这条路的可行性，很大程度上取决于一个假设：**用户会在一个模型上稳定停留足够长的时间**。

从目前的趋势看，这个假设有一定支撑——不少企业用户在工作流跑通之后，并不急着追最新版本。但 AI 模型迭代的速度依然是这个商业模式最大的不确定性。

---

## 五、这件事的更大意义

Taalas 让我想到的不只是"又一家挑战英伟达的公司"。

这件事本身有两个层次值得关注：

**第一层：推理成本正在被打穿，从两个方向同时进行。**

DeepSeek 从训练端下手——用更聪明的算法，让模型用更少的算力训练出来，把推理的计算量降下来。Taalas 从硬件端下手——让同样的模型，在专用硬件上跑得极其便宜。两条路，同一个方向：让每一个 AI token 的成本趋近于零。

**第二层：25 个人，3000 万美元，打穿了一个几百亿美元的市场的护城河。**

Tenstorrent 做通用 AI 芯片，融了 7 亿美元，估值 32 亿。Taalas 放弃通用性，只做一件事做到极致，3000 万美元出了一个跑赢旗舰 GPU 10 倍的产品。资源和效率之间的关系，在这个案例里看得特别清楚。

Bajic 自己说：**越聚越窄，越聚越深，越聚越快。** 这句话不只是芯片设计的原则，也像是一种工程哲学。

---

## 六、现在能用吗？

可以。

Taalas 已经开放了两个入口：

- **演示体验**：[chatjimmy.ai](https://chatjimmy.ai)，直接感受 17,000 tokens/秒是什么速度
- **API 申请**：[taalas.com/api-request-form](https://taalas.com/api-request-form/)，开发者可以申请接入测试

产品路线图：

| 时间 | 里程碑 |
|------|--------|
| 现在 | HC1：Llama 3.1 8B，API 已开放 |
| 2026 夏 | HC1 扩展版：支持 20B 参数模型 |
| 2026 年底 | HC2：前沿模型（Llama 4 / DeepSeek），多卡流水线，FP4 精度 |
| 12 个月内 | 千亿参数级前沿 LLM 支持 |

---

## 尾声

我习惯对"颠覆 NVIDIA"的新闻保持怀疑。每隔几个月就有一家。

但 Taalas 的这件事让我停下来认真想了一下。不是因为 17,000 这个数字，而是因为那个技术决策的逻辑链条——放弃通用性，否定冯·诺依曼，把模型变成硬件本身——这个思路是自洽的，数字是可以被独立验证的，芯片是真实量产的。

它不会取代 GPU，因为训练永远需要通用算力。但在"大规模稳定推理"这个场景里，它可能真的找到了一条完全不同的路。

结局还没写完。但开头确实很精彩。

---

**参考资料**

- Taalas 官方博客：[The Path to Ubiquitous AI](https://taalas.com/the-path-to-ubiquitous-ai/)
- NextPlatform 深度报道：[Taalas Etches AI Models Onto Transistors](https://www.nextplatform.com/2026/02/19/taalas-etches-ai-models-onto-transistors-to-rocket-boost-inference/)
- 钛媒体：["邪修"AI芯片的Taalas，成色如何？](https://www.tmtpost.com/7887595.html)
- 傅盛：[一个疯子，造出便宜100倍的AI芯片](https://www.163.com/dy/article/KMFA798N055685JJ.html)
- Hacker News 讨论：[The path to ubiquitous AI (17k tokens/sec)](https://news.ycombinator.com/item?id=47086181)
